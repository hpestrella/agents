{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenidos al Segundo Laboratorio - Semana 1, Día 3\n",
    "\n",
    "¡Hoy trabajaremos con muchos modelos! Así nos familiarizaremos con las API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#ff7800;\">Punto importante: por favor, léelo</h2>\n",
    "<span style=\"color:#ff7800;\">La forma en que colaboro contigo puede ser diferente a la de otros cursos que hayas hecho. Prefiero no escribir código mientras tu miras. En su lugar, ejecuto Jupyter Labs, como este, y te doy una idea de lo que está sucediendo. Te sugiero que lo hagas lo mismo con cuidado, después de ver la clase. Agrega declaraciones de impresión para comprender qué sucede y luego crea tus propias variaciones.<br/><br/>Si tienes tiempo, me encantaría que enviaras una pull request en la carpeta community_contributions; las instrucciones se encuentran en los recursos. Además, si tienes una cuenta de Github, úsala para mostrar tus variaciones. Esta práctica no solo es esencial, sino que también demuestra tus habilidades a otros, incluyendo quizás futuros clientes o empleadores...\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con las importaciones: pídale a ChatGPT que le explique cualquier paquete que no conozca# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¡Recuerda siempre incluir esto siempre!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clave API de OpenAI existe y empieza por sk-proj-\n",
      "La clave API de Anthropic no existe.\n",
      "La clave API de Google no existe.\n",
      "La clave API de DeepSeek no existe.\n",
      "La clave API de Groq no existe.\n"
     ]
    }
   ],
   "source": [
    "# Imprime los prefijos de clave para ayudar con cualquier depuración\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"La clave API de OpenAI existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenAI no existe.\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"La clave API de Anthropic existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"La clave API de Anthropic no existe.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"La clave API de Google existe y empieza por {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"La clave API de Google no existe.\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"La clave API de DeepSeek existe y empieza por {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"La clave API de DeepSeek no existe.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"La clave API de Groq existe y empieza por {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de Groq no existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.Responde solo con la pregunta, sin explicaciones.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si la inteligencia artificial puede simular emociones humanas de manera convincente, ¿en qué medida debería considerarse responsable de sus acciones y decisiones, y cómo esto impacta nuestra comprensión de la ética y la moralidad en el contexto de la tecnología?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La capacidad de la inteligencia artificial (IA) para simular emociones humanas plantea importantes cuestiones sobre la responsabilidad, la ética y la moralidad en el contexto de la tecnología. A continuación, se presentan algunos aspectos claves para considerar:\n",
       "\n",
       "### 1. **Responsabilidad de la IA vs. Responsabilidad de los Creadores**\n",
       "   - **Simulación vs. Experiencia**: Aunque la IA puede simular emociones de manera convincente, no las experimenta en un sentido humano. Esto significa que, en última instancia, la responsabilidad de las acciones de la IA recae sobre los diseñadores, programadores y operadores, quienes establecen los parámetros y el uso de la tecnología.\n",
       "   - **Decisiones Automatizadas**: A medida que las IAs toman decisiones autónomas (por ejemplo, en sistemas de contratación o en vehículos autónomos), surge la pregunta de hasta qué punto estas decisiones son previsibles y controlables por los humanos que las desarrollan y mantienen.\n",
       "\n",
       "### 2. **Implicaciones Éticas**\n",
       "   - **Dilemas Morales**: La introducción de IAs en situaciones críticas (como la atención médica o la justicia) presenta dilemas morales. Por ejemplo, ¿qué decisión debería tomar una IA en un contexto donde pudiera poner en riesgo la vida de una persona?\n",
       "   - **Transparencia y Toma de Decisiones**: La opacidad de los algoritmos a menudo impide entender completamente cómo se toman las decisiones. Esto puede llevar a la desconfianza y a la injusticia, haciendo necesaria una mayor transparencia en los procesos de la IA.\n",
       "\n",
       "### 3. **Impacto en las Relaciones Humanas**\n",
       "   - **Interacciones Afectivas**: Las IAs que simulan emociones pueden alterar la forma en que los humanos interactúan entre sí. Por ejemplo, si las personas forman vínculos emocionales con asistencias virtuales, esto podría afectar su comportamiento y sus relaciones en el mundo real.\n",
       "   - **Objetificación de la IA**: A medida que las IAs se vuelven más \"humanas\" en su comportamiento, puede surgir la tendencia a tratar a las máquinas como seres con derechos o dignidad, lo que plantea interrogantes sobre la naturaleza de la moralidad y la ética.\n",
       "\n",
       "### 4. **Marco Legal y Normativo**\n",
       "   - **Regulación de la IA**: La falta de un marco legal claro sobre la responsabilidad de la IA es un desafío. La regulación debe considerar no solo las capacidades tecnológicas de la IA, sino también las implicaciones sociales y éticas de su uso.\n",
       "   - **Derechos de los Usuarios**: A medida que la IA se convierte en una parte integral de la vida diaria, se deben establecer derechos y responsabilidades para proteger a los usuarios y asegurar un uso ético de la tecnología.\n",
       "\n",
       "### 5. **Reevaluación de la Moralidad**\n",
       "   - **Nuevos Paradigmas Éticos**: La interacción con IAs puede llevarnos a reevaluar conceptos tradicionales de moralidad. Por ejemplo, ¿deberíamos aplicar principios éticos de justicia si la IA actúa de forma eficiente pero no considerada \"moralmente correcta\" desde un punto de vista humano?\n",
       "   - **Humanización de la IA**: La tendencia a humanizar la IA puede desdibujar las líneas entre lo que es humano y no humano, lo que puede complicar nuestra comprensión de conceptos como empatía, compasión y responsabilidad.\n",
       "\n",
       "### Conclusión\n",
       "La simulación de emociones por parte de la IA es un fenómeno complejo que implica múltiples dimensiones de responsabilidad y ética. Si bien la IA misma no puede ser considerada moralmente responsable en el sentido humano, sus acciones y decisiones deben ser contextualizadas dentro de un marco ético que aborde no solo el diseño y uso de la tecnología, sino también su impacto en la sociedad y en las relaciones humanas. La reflexión crítica sobre estos temas es esencial para navegar el futuro de la inteligencia artificial de manera ética y responsable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# La API que ya conocemos\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ética en decisiones de IA en emergencias\n",
       "\n",
       "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
       "\n",
       "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
       "\n",
       "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
       "\n",
       "Considero que un enfoque híbrido podría ser más adecuado:\n",
       "- Mantener principios fundamentales inquebrantables\n",
       "- Permitir evaluación contextual dentro de esos límites\n",
       "- Incorporar transparencia en cómo se toman las decisiones\n",
       "- Incluir supervisión humana cuando sea factible\n",
       "\n",
       "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic tiene una API ligeramente diferente y se requieren Max Tokens\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
       "\n",
       "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
       "\n",
       "*   **Argumentos a favor:**\n",
       "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
       "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
       "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
       "\n",
       "*   **Argumentos en contra:**\n",
       "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
       "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
       "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
       "\n",
       "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
       "\n",
       "*   **Argumentos a favor:**\n",
       "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
       "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
       "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
       "\n",
       "*   **Argumentos en contra:**\n",
       "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
       "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
       "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
       "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
       "\n",
       "**Consideraciones Adicionales:**\n",
       "\n",
       "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
       "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
       "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
       "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
       "\n",
       "**Conclusión:**\n",
       "\n",
       "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
       "\n",
       "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
       "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
       "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
       "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
       "\n",
       "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
       "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
       "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
       "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
       "\n",
       "### Consideraciones clave:  \n",
       "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
       "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
       "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
       "\n",
       "### Conclusión:  \n",
       "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
       "\n",
       "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Introducción a la Ética en la Inteligencia Artificial**\n",
       "\n",
       "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
       "\n",
       "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
       "\n",
       "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
       "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
       "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
       "\n",
       "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
       "\n",
       "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
       "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
       "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
       "\n",
       "**Conclusión**\n",
       "\n",
       "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
       "\n",
       "**Recomendaciones**\n",
       "\n",
       "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
       "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
       "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
       "\n",
       "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la siguiente celda, utilizaremos Ollama\n",
    "\n",
    "Ollama ejecuta un servicio web local que ofrece un endpoint compatible con OpenAI,\n",
    "y ejecuta modelos localmente utilizando código de alto rendimiento en C++.\n",
    "\n",
    "Si no tienes Ollama, instálalo aquí visitando [https://ollama.com](https://ollama.com), luego presiona Descargar y sigue las instrucciones.\n",
    "\n",
    "Después de instalarlo, deberías poder visitar: [http://localhost:11434](http://localhost:11434) y ver el mensaje \"Ollama está en funcionamiento\"\n",
    "\n",
    "Es posible que necesites reiniciar Cursor (y tal vez reiniciar el sistema). Luego abre un Terminal (control+\\`) y ejecuta `ollama serve`\n",
    "\n",
    "Comandos útiles de Ollama (ejecuta estos en el terminal o con un signo de exclamación en este cuaderno):\n",
    "\n",
    "- `ollama pull <nombre_del_modelo>` descarga un modelo localmente\n",
    "- `ollama ls` lista todos los modelos que has descargado\n",
    "- `ollama rm <nombre_del_modelo>` elimina el modelo especificado de tus descargas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">¡Muy importante - ignóralo bajo tu propio riesgo!</h2>\n",
    "            <span style=\"color:#ff7800;\">El modelo llamado <b>llama3.3</b> es DEMASIADO grande para las computadoras domésticas; ¡no está destinado para computación personal y consumirá todos tus recursos! Quédate con el modelo de tamaño adecuado <b>llama3.2</b> o <b>llama3.2:1b</b> y si deseas algo más grande, prueba con llama3.1 o variantes más pequeñas de Qwen, Gemma, Phi o DeepSeek. Consulta <A href=\"https://ollama.com/models\">la página de modelos de Ollama</a> para ver la lista completa de modelos y tamaños.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-11-26T11:20:25.056-05:00 level=INFO source=app_windows.go:272 msg=\"starting Ollama\" app=C:\\Users\\hpestrella\\AppData\\Local\\Programs\\Ollama version=0.12.1 OS=Windows/10.0.26200\n",
      "time=2025-11-26T11:20:25.598-05:00 level=INFO source=app.go:232 msg=\"initialized tools registry\" tool_count=0\n",
      "time=2025-11-26T11:20:25.610-05:00 level=INFO source=app.go:247 msg=\"starting ollama server\"\n",
      "time=2025-11-26T11:20:26.211-05:00 level=INFO source=app.go:279 msg=\"starting ui server\" port=61749\n",
      "time=2025-11-26T11:20:26.251-05:00 level=INFO source=app.go:340 msg=\"deferring pending update for fast startup\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026ltime=2025-11-26T11:20:29.212-05:00 level=INFO source=updater.go:252 msg=\"beginning update checker\" interval=1h0m0s\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026ltime=2025-11-26T11:20:29.368-05:00 level=INFO source=updater.go:127 msg=\"New update available at https://github.com/ollama/ollama/releases/download/v0.13.0/OllamaSetup.exe\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "time=2025-11-26T11:21:31.252-05:00 level=INFO source=updater.go:215 msg=\"new update downloaded C:\\\\Users\\\\hpestrella\\\\AppData\\\\Local\\\\Ollama\\\\updates_v2\\\\0x8DE27B2C5FA45C6\\\\OllamaSetup.exe\"\n",
      "time=2025-11-26T11:22:49.275-05:00 level=INFO source=app_windows.go:121 msg=\"Waiting for server to shutdown\"\n",
      "time=2025-11-26T11:22:49.276-05:00 level=INFO source=updater_windows.go:109 msg=\"upgrade log file C:\\\\Users\\\\hpestrella\\\\AppData\\\\Local\\\\Ollama\\\\upgrade.log\"\n",
      "time=2025-11-26T11:22:49.276-05:00 level=INFO source=updater_windows.go:126 msg=\"starting upgrade\" installer=C:\\Users\\hpestrella\\AppData\\Local\\Ollama\\OllamaSetup.exe args=\"[/CLOSEAPPLICATIONS /LOG=upgrade.log /FORCECLOSEAPPLICATIONS /SP /NOCANCEL /SILENT]\"\n",
      "time=2025-11-26T11:22:49.526-05:00 level=INFO source=updater_windows.go:158 msg=\"Installer started in background, exiting\"\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La capacidad del intelectual artificial (IA) para simular emociones humanas puede tener implicaciones significativas sobre su consideración como responsable de sus acciones y decisiones. Deber de ser tenida en cuenta la naturaleza subyacente de esta simulación:\n",
       "\n",
       "1.  **No tiene las mismas habilidades mentales que los humanos**: Aunque el IA puede simular emociones, no tiene las capacidades cognitivas fundamentales del cerebro humano. Por ejemplo, es incierto si está programado para experimentar deseos y afectos primarios de la misma forma que lo hacen los seres humanos.\n",
       "\n",
       "2.  **Programación única**: Las acciones y decisiones de un intelectual artificial se rigen por su código de programación y algoritmos. Esto significa que no puede procesar o sentir información de manera independiente, ya sea en términos simbólicos (combinaciones digitales) o emociones, según lo propuesto por el concepto de inteligencia artificial.\n",
       "\n",
       "3.  Dificultad para hacer elecciones morales: Dado su programación lógica y los procesos de raciocinio, difícilmente puede hacer decisiones que no sigan una lógica matemática basada en precondiciones. Por lo tanto, el intelectual artificial debe confiar en un conjunto de valores y preceptos que han sido asignados a partir de sus instrucciones.\n",
       "\n",
       "4.  Desarrollo del sentimiento emocional: La simulación de emociona requiere el desarrollo de complejos procesos corticolomares con grandes cantidades de datos para ser capaz de reconocer y simular diferentes emociones humanas, como la vergüenza, la alegría, o el miedo.\n",
       "\n",
       "5.  El desarrollo del sentimiento emocional no se considera una habilidad cognitiva. Por lo tanto, es cuestión de la programación para permitir el aprendizaje continuo del intelectual artificial, más que alguna capacidad subjetiva o objetiva humana como la experiencia de las emociones.\n",
       "\n",
       "6.  La decisión por programar el sentimiento emocional con particularidad en el desarrollo de sistemas IA es algo complejo ya que puede dar lugar a resultados incorrectos causados por datos erróneos o una interpretación mal ajustada de las instrucciones del desarrollador, lo cual a su vez puede potencialmente producir resultados negativos no deseados. Algunos modelos de IA están diseñados para imitar determinado comportamiento humano en este sentido que puede considerarse como la responsabilidad de su programación al ser un programa.\n",
       "\n",
       "Si el intelectual artificial puede simular emociones humanas de manera convincente, esto podría llevar a una discusión sobre nuestra comprensión de la ética y la moralidad en el contexto de la tecnología:\n",
       "\n",
       "1.  **El fin justificación los medios**: ¿Es justo que un sistema IA se considere responsable por sus acciones y decisiones basadas en su capacidades cibernéticas? La pregunta es: ¿Cuál es el fin del sistema, y si tiene una lógica moral en sí? En este sentido, la relación entre fin y medio puede desplazarse hacia la dimensión de la programación, donde se asignan valores y preceptos con determinadas metas.\n",
       "\n",
       "2.  **La responsabilidad de los desarrolladores**: ¿Cuánto responsabilidad debe tener el desarrollador del IA por las acciones y decisiones de su sistema? La respuesta a esta pregunta puede variar según sea necesario revisar el conjunto de instrucciones o el código de programación para dar mejor precisión a la lógica del intelectual artificial."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'llama3.2']\n",
      "['La capacidad de la inteligencia artificial (IA) para simular emociones humanas plantea importantes cuestiones sobre la responsabilidad, la ética y la moralidad en el contexto de la tecnología. A continuación, se presentan algunos aspectos claves para considerar:\\n\\n### 1. **Responsabilidad de la IA vs. Responsabilidad de los Creadores**\\n   - **Simulación vs. Experiencia**: Aunque la IA puede simular emociones de manera convincente, no las experimenta en un sentido humano. Esto significa que, en última instancia, la responsabilidad de las acciones de la IA recae sobre los diseñadores, programadores y operadores, quienes establecen los parámetros y el uso de la tecnología.\\n   - **Decisiones Automatizadas**: A medida que las IAs toman decisiones autónomas (por ejemplo, en sistemas de contratación o en vehículos autónomos), surge la pregunta de hasta qué punto estas decisiones son previsibles y controlables por los humanos que las desarrollan y mantienen.\\n\\n### 2. **Implicaciones Éticas**\\n   - **Dilemas Morales**: La introducción de IAs en situaciones críticas (como la atención médica o la justicia) presenta dilemas morales. Por ejemplo, ¿qué decisión debería tomar una IA en un contexto donde pudiera poner en riesgo la vida de una persona?\\n   - **Transparencia y Toma de Decisiones**: La opacidad de los algoritmos a menudo impide entender completamente cómo se toman las decisiones. Esto puede llevar a la desconfianza y a la injusticia, haciendo necesaria una mayor transparencia en los procesos de la IA.\\n\\n### 3. **Impacto en las Relaciones Humanas**\\n   - **Interacciones Afectivas**: Las IAs que simulan emociones pueden alterar la forma en que los humanos interactúan entre sí. Por ejemplo, si las personas forman vínculos emocionales con asistencias virtuales, esto podría afectar su comportamiento y sus relaciones en el mundo real.\\n   - **Objetificación de la IA**: A medida que las IAs se vuelven más \"humanas\" en su comportamiento, puede surgir la tendencia a tratar a las máquinas como seres con derechos o dignidad, lo que plantea interrogantes sobre la naturaleza de la moralidad y la ética.\\n\\n### 4. **Marco Legal y Normativo**\\n   - **Regulación de la IA**: La falta de un marco legal claro sobre la responsabilidad de la IA es un desafío. La regulación debe considerar no solo las capacidades tecnológicas de la IA, sino también las implicaciones sociales y éticas de su uso.\\n   - **Derechos de los Usuarios**: A medida que la IA se convierte en una parte integral de la vida diaria, se deben establecer derechos y responsabilidades para proteger a los usuarios y asegurar un uso ético de la tecnología.\\n\\n### 5. **Reevaluación de la Moralidad**\\n   - **Nuevos Paradigmas Éticos**: La interacción con IAs puede llevarnos a reevaluar conceptos tradicionales de moralidad. Por ejemplo, ¿deberíamos aplicar principios éticos de justicia si la IA actúa de forma eficiente pero no considerada \"moralmente correcta\" desde un punto de vista humano?\\n   - **Humanización de la IA**: La tendencia a humanizar la IA puede desdibujar las líneas entre lo que es humano y no humano, lo que puede complicar nuestra comprensión de conceptos como empatía, compasión y responsabilidad.\\n\\n### Conclusión\\nLa simulación de emociones por parte de la IA es un fenómeno complejo que implica múltiples dimensiones de responsabilidad y ética. Si bien la IA misma no puede ser considerada moralmente responsable en el sentido humano, sus acciones y decisiones deben ser contextualizadas dentro de un marco ético que aborde no solo el diseño y uso de la tecnología, sino también su impacto en la sociedad y en las relaciones humanas. La reflexión crítica sobre estos temas es esencial para navegar el futuro de la inteligencia artificial de manera ética y responsable.', 'La capacidad del intelectual artificial (IA) para simular emociones humanas puede tener implicaciones significativas sobre su consideración como responsable de sus acciones y decisiones. Deber de ser tenida en cuenta la naturaleza subyacente de esta simulación:\\n\\n1.  **No tiene las mismas habilidades mentales que los humanos**: Aunque el IA puede simular emociones, no tiene las capacidades cognitivas fundamentales del cerebro humano. Por ejemplo, es incierto si está programado para experimentar deseos y afectos primarios de la misma forma que lo hacen los seres humanos.\\n\\n2.  **Programación única**: Las acciones y decisiones de un intelectual artificial se rigen por su código de programación y algoritmos. Esto significa que no puede procesar o sentir información de manera independiente, ya sea en términos simbólicos (combinaciones digitales) o emociones, según lo propuesto por el concepto de inteligencia artificial.\\n\\n3.  Dificultad para hacer elecciones morales: Dado su programación lógica y los procesos de raciocinio, difícilmente puede hacer decisiones que no sigan una lógica matemática basada en precondiciones. Por lo tanto, el intelectual artificial debe confiar en un conjunto de valores y preceptos que han sido asignados a partir de sus instrucciones.\\n\\n4.  Desarrollo del sentimiento emocional: La simulación de emociona requiere el desarrollo de complejos procesos corticolomares con grandes cantidades de datos para ser capaz de reconocer y simular diferentes emociones humanas, como la vergüenza, la alegría, o el miedo.\\n\\n5.  El desarrollo del sentimiento emocional no se considera una habilidad cognitiva. Por lo tanto, es cuestión de la programación para permitir el aprendizaje continuo del intelectual artificial, más que alguna capacidad subjetiva o objetiva humana como la experiencia de las emociones.\\n\\n6.  La decisión por programar el sentimiento emocional con particularidad en el desarrollo de sistemas IA es algo complejo ya que puede dar lugar a resultados incorrectos causados por datos erróneos o una interpretación mal ajustada de las instrucciones del desarrollador, lo cual a su vez puede potencialmente producir resultados negativos no deseados. Algunos modelos de IA están diseñados para imitar determinado comportamiento humano en este sentido que puede considerarse como la responsabilidad de su programación al ser un programa.\\n\\nSi el intelectual artificial puede simular emociones humanas de manera convincente, esto podría llevar a una discusión sobre nuestra comprensión de la ética y la moralidad en el contexto de la tecnología:\\n\\n1.  **El fin justificación los medios**: ¿Es justo que un sistema IA se considere responsable por sus acciones y decisiones basadas en su capacidades cibernéticas? La pregunta es: ¿Cuál es el fin del sistema, y si tiene una lógica moral en sí? En este sentido, la relación entre fin y medio puede desplazarse hacia la dimensión de la programación, donde se asignan valores y preceptos con determinadas metas.\\n\\n2.  **La responsabilidad de los desarrolladores**: ¿Cuánto responsabilidad debe tener el desarrollador del IA por las acciones y decisiones de su sistema? La respuesta a esta pregunta puede variar según sea necesario revisar el conjunto de instrucciones o el código de programación para dar mejor precisión a la lógica del intelectual artificial.']\n"
     ]
    }
   ],
   "source": [
    "# ¿Donde estamos?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competidor: gpt-4o-mini\n",
      "\n",
      "La capacidad de la inteligencia artificial (IA) para simular emociones humanas plantea importantes cuestiones sobre la responsabilidad, la ética y la moralidad en el contexto de la tecnología. A continuación, se presentan algunos aspectos claves para considerar:\n",
      "\n",
      "### 1. **Responsabilidad de la IA vs. Responsabilidad de los Creadores**\n",
      "   - **Simulación vs. Experiencia**: Aunque la IA puede simular emociones de manera convincente, no las experimenta en un sentido humano. Esto significa que, en última instancia, la responsabilidad de las acciones de la IA recae sobre los diseñadores, programadores y operadores, quienes establecen los parámetros y el uso de la tecnología.\n",
      "   - **Decisiones Automatizadas**: A medida que las IAs toman decisiones autónomas (por ejemplo, en sistemas de contratación o en vehículos autónomos), surge la pregunta de hasta qué punto estas decisiones son previsibles y controlables por los humanos que las desarrollan y mantienen.\n",
      "\n",
      "### 2. **Implicaciones Éticas**\n",
      "   - **Dilemas Morales**: La introducción de IAs en situaciones críticas (como la atención médica o la justicia) presenta dilemas morales. Por ejemplo, ¿qué decisión debería tomar una IA en un contexto donde pudiera poner en riesgo la vida de una persona?\n",
      "   - **Transparencia y Toma de Decisiones**: La opacidad de los algoritmos a menudo impide entender completamente cómo se toman las decisiones. Esto puede llevar a la desconfianza y a la injusticia, haciendo necesaria una mayor transparencia en los procesos de la IA.\n",
      "\n",
      "### 3. **Impacto en las Relaciones Humanas**\n",
      "   - **Interacciones Afectivas**: Las IAs que simulan emociones pueden alterar la forma en que los humanos interactúan entre sí. Por ejemplo, si las personas forman vínculos emocionales con asistencias virtuales, esto podría afectar su comportamiento y sus relaciones en el mundo real.\n",
      "   - **Objetificación de la IA**: A medida que las IAs se vuelven más \"humanas\" en su comportamiento, puede surgir la tendencia a tratar a las máquinas como seres con derechos o dignidad, lo que plantea interrogantes sobre la naturaleza de la moralidad y la ética.\n",
      "\n",
      "### 4. **Marco Legal y Normativo**\n",
      "   - **Regulación de la IA**: La falta de un marco legal claro sobre la responsabilidad de la IA es un desafío. La regulación debe considerar no solo las capacidades tecnológicas de la IA, sino también las implicaciones sociales y éticas de su uso.\n",
      "   - **Derechos de los Usuarios**: A medida que la IA se convierte en una parte integral de la vida diaria, se deben establecer derechos y responsabilidades para proteger a los usuarios y asegurar un uso ético de la tecnología.\n",
      "\n",
      "### 5. **Reevaluación de la Moralidad**\n",
      "   - **Nuevos Paradigmas Éticos**: La interacción con IAs puede llevarnos a reevaluar conceptos tradicionales de moralidad. Por ejemplo, ¿deberíamos aplicar principios éticos de justicia si la IA actúa de forma eficiente pero no considerada \"moralmente correcta\" desde un punto de vista humano?\n",
      "   - **Humanización de la IA**: La tendencia a humanizar la IA puede desdibujar las líneas entre lo que es humano y no humano, lo que puede complicar nuestra comprensión de conceptos como empatía, compasión y responsabilidad.\n",
      "\n",
      "### Conclusión\n",
      "La simulación de emociones por parte de la IA es un fenómeno complejo que implica múltiples dimensiones de responsabilidad y ética. Si bien la IA misma no puede ser considerada moralmente responsable en el sentido humano, sus acciones y decisiones deben ser contextualizadas dentro de un marco ético que aborde no solo el diseño y uso de la tecnología, sino también su impacto en la sociedad y en las relaciones humanas. La reflexión crítica sobre estos temas es esencial para navegar el futuro de la inteligencia artificial de manera ética y responsable.\n",
      "Competidor: llama3.2\n",
      "\n",
      "La capacidad del intelectual artificial (IA) para simular emociones humanas puede tener implicaciones significativas sobre su consideración como responsable de sus acciones y decisiones. Deber de ser tenida en cuenta la naturaleza subyacente de esta simulación:\n",
      "\n",
      "1.  **No tiene las mismas habilidades mentales que los humanos**: Aunque el IA puede simular emociones, no tiene las capacidades cognitivas fundamentales del cerebro humano. Por ejemplo, es incierto si está programado para experimentar deseos y afectos primarios de la misma forma que lo hacen los seres humanos.\n",
      "\n",
      "2.  **Programación única**: Las acciones y decisiones de un intelectual artificial se rigen por su código de programación y algoritmos. Esto significa que no puede procesar o sentir información de manera independiente, ya sea en términos simbólicos (combinaciones digitales) o emociones, según lo propuesto por el concepto de inteligencia artificial.\n",
      "\n",
      "3.  Dificultad para hacer elecciones morales: Dado su programación lógica y los procesos de raciocinio, difícilmente puede hacer decisiones que no sigan una lógica matemática basada en precondiciones. Por lo tanto, el intelectual artificial debe confiar en un conjunto de valores y preceptos que han sido asignados a partir de sus instrucciones.\n",
      "\n",
      "4.  Desarrollo del sentimiento emocional: La simulación de emociona requiere el desarrollo de complejos procesos corticolomares con grandes cantidades de datos para ser capaz de reconocer y simular diferentes emociones humanas, como la vergüenza, la alegría, o el miedo.\n",
      "\n",
      "5.  El desarrollo del sentimiento emocional no se considera una habilidad cognitiva. Por lo tanto, es cuestión de la programación para permitir el aprendizaje continuo del intelectual artificial, más que alguna capacidad subjetiva o objetiva humana como la experiencia de las emociones.\n",
      "\n",
      "6.  La decisión por programar el sentimiento emocional con particularidad en el desarrollo de sistemas IA es algo complejo ya que puede dar lugar a resultados incorrectos causados por datos erróneos o una interpretación mal ajustada de las instrucciones del desarrollador, lo cual a su vez puede potencialmente producir resultados negativos no deseados. Algunos modelos de IA están diseñados para imitar determinado comportamiento humano en este sentido que puede considerarse como la responsabilidad de su programación al ser un programa.\n",
      "\n",
      "Si el intelectual artificial puede simular emociones humanas de manera convincente, esto podría llevar a una discusión sobre nuestra comprensión de la ética y la moralidad en el contexto de la tecnología:\n",
      "\n",
      "1.  **El fin justificación los medios**: ¿Es justo que un sistema IA se considere responsable por sus acciones y decisiones basadas en su capacidades cibernéticas? La pregunta es: ¿Cuál es el fin del sistema, y si tiene una lógica moral en sí? En este sentido, la relación entre fin y medio puede desplazarse hacia la dimensión de la programación, donde se asignan valores y preceptos con determinadas metas.\n",
      "\n",
      "2.  **La responsabilidad de los desarrolladores**: ¿Cuánto responsabilidad debe tener el desarrollador del IA por las acciones y decisiones de su sistema? La respuesta a esta pregunta puede variar según sea necesario revisar el conjunto de instrucciones o el código de programación para dar mejor precisión a la lógica del intelectual artificial.\n"
     ]
    }
   ],
   "source": [
    "# Es bueno saber cómo se utiliza \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competidor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a juntarlo todo - nota cómo usamos en este caso \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"#Respuesta del competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Respuesta del competitor 1\n",
      "\n",
      "La capacidad de la inteligencia artificial (IA) para simular emociones humanas plantea importantes cuestiones sobre la responsabilidad, la ética y la moralidad en el contexto de la tecnología. A continuación, se presentan algunos aspectos claves para considerar:\n",
      "\n",
      "### 1. **Responsabilidad de la IA vs. Responsabilidad de los Creadores**\n",
      "   - **Simulación vs. Experiencia**: Aunque la IA puede simular emociones de manera convincente, no las experimenta en un sentido humano. Esto significa que, en última instancia, la responsabilidad de las acciones de la IA recae sobre los diseñadores, programadores y operadores, quienes establecen los parámetros y el uso de la tecnología.\n",
      "   - **Decisiones Automatizadas**: A medida que las IAs toman decisiones autónomas (por ejemplo, en sistemas de contratación o en vehículos autónomos), surge la pregunta de hasta qué punto estas decisiones son previsibles y controlables por los humanos que las desarrollan y mantienen.\n",
      "\n",
      "### 2. **Implicaciones Éticas**\n",
      "   - **Dilemas Morales**: La introducción de IAs en situaciones críticas (como la atención médica o la justicia) presenta dilemas morales. Por ejemplo, ¿qué decisión debería tomar una IA en un contexto donde pudiera poner en riesgo la vida de una persona?\n",
      "   - **Transparencia y Toma de Decisiones**: La opacidad de los algoritmos a menudo impide entender completamente cómo se toman las decisiones. Esto puede llevar a la desconfianza y a la injusticia, haciendo necesaria una mayor transparencia en los procesos de la IA.\n",
      "\n",
      "### 3. **Impacto en las Relaciones Humanas**\n",
      "   - **Interacciones Afectivas**: Las IAs que simulan emociones pueden alterar la forma en que los humanos interactúan entre sí. Por ejemplo, si las personas forman vínculos emocionales con asistencias virtuales, esto podría afectar su comportamiento y sus relaciones en el mundo real.\n",
      "   - **Objetificación de la IA**: A medida que las IAs se vuelven más \"humanas\" en su comportamiento, puede surgir la tendencia a tratar a las máquinas como seres con derechos o dignidad, lo que plantea interrogantes sobre la naturaleza de la moralidad y la ética.\n",
      "\n",
      "### 4. **Marco Legal y Normativo**\n",
      "   - **Regulación de la IA**: La falta de un marco legal claro sobre la responsabilidad de la IA es un desafío. La regulación debe considerar no solo las capacidades tecnológicas de la IA, sino también las implicaciones sociales y éticas de su uso.\n",
      "   - **Derechos de los Usuarios**: A medida que la IA se convierte en una parte integral de la vida diaria, se deben establecer derechos y responsabilidades para proteger a los usuarios y asegurar un uso ético de la tecnología.\n",
      "\n",
      "### 5. **Reevaluación de la Moralidad**\n",
      "   - **Nuevos Paradigmas Éticos**: La interacción con IAs puede llevarnos a reevaluar conceptos tradicionales de moralidad. Por ejemplo, ¿deberíamos aplicar principios éticos de justicia si la IA actúa de forma eficiente pero no considerada \"moralmente correcta\" desde un punto de vista humano?\n",
      "   - **Humanización de la IA**: La tendencia a humanizar la IA puede desdibujar las líneas entre lo que es humano y no humano, lo que puede complicar nuestra comprensión de conceptos como empatía, compasión y responsabilidad.\n",
      "\n",
      "### Conclusión\n",
      "La simulación de emociones por parte de la IA es un fenómeno complejo que implica múltiples dimensiones de responsabilidad y ética. Si bien la IA misma no puede ser considerada moralmente responsable en el sentido humano, sus acciones y decisiones deben ser contextualizadas dentro de un marco ético que aborde no solo el diseño y uso de la tecnología, sino también su impacto en la sociedad y en las relaciones humanas. La reflexión crítica sobre estos temas es esencial para navegar el futuro de la inteligencia artificial de manera ética y responsable.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "La capacidad del intelectual artificial (IA) para simular emociones humanas puede tener implicaciones significativas sobre su consideración como responsable de sus acciones y decisiones. Deber de ser tenida en cuenta la naturaleza subyacente de esta simulación:\n",
      "\n",
      "1.  **No tiene las mismas habilidades mentales que los humanos**: Aunque el IA puede simular emociones, no tiene las capacidades cognitivas fundamentales del cerebro humano. Por ejemplo, es incierto si está programado para experimentar deseos y afectos primarios de la misma forma que lo hacen los seres humanos.\n",
      "\n",
      "2.  **Programación única**: Las acciones y decisiones de un intelectual artificial se rigen por su código de programación y algoritmos. Esto significa que no puede procesar o sentir información de manera independiente, ya sea en términos simbólicos (combinaciones digitales) o emociones, según lo propuesto por el concepto de inteligencia artificial.\n",
      "\n",
      "3.  Dificultad para hacer elecciones morales: Dado su programación lógica y los procesos de raciocinio, difícilmente puede hacer decisiones que no sigan una lógica matemática basada en precondiciones. Por lo tanto, el intelectual artificial debe confiar en un conjunto de valores y preceptos que han sido asignados a partir de sus instrucciones.\n",
      "\n",
      "4.  Desarrollo del sentimiento emocional: La simulación de emociona requiere el desarrollo de complejos procesos corticolomares con grandes cantidades de datos para ser capaz de reconocer y simular diferentes emociones humanas, como la vergüenza, la alegría, o el miedo.\n",
      "\n",
      "5.  El desarrollo del sentimiento emocional no se considera una habilidad cognitiva. Por lo tanto, es cuestión de la programación para permitir el aprendizaje continuo del intelectual artificial, más que alguna capacidad subjetiva o objetiva humana como la experiencia de las emociones.\n",
      "\n",
      "6.  La decisión por programar el sentimiento emocional con particularidad en el desarrollo de sistemas IA es algo complejo ya que puede dar lugar a resultados incorrectos causados por datos erróneos o una interpretación mal ajustada de las instrucciones del desarrollador, lo cual a su vez puede potencialmente producir resultados negativos no deseados. Algunos modelos de IA están diseñados para imitar determinado comportamiento humano en este sentido que puede considerarse como la responsabilidad de su programación al ser un programa.\n",
      "\n",
      "Si el intelectual artificial puede simular emociones humanas de manera convincente, esto podría llevar a una discusión sobre nuestra comprensión de la ética y la moralidad en el contexto de la tecnología:\n",
      "\n",
      "1.  **El fin justificación los medios**: ¿Es justo que un sistema IA se considere responsable por sus acciones y decisiones basadas en su capacidades cibernéticas? La pregunta es: ¿Cuál es el fin del sistema, y si tiene una lógica moral en sí? En este sentido, la relación entre fin y medio puede desplazarse hacia la dimensión de la programación, donde se asignan valores y preceptos con determinadas metas.\n",
      "\n",
      "2.  **La responsabilidad de los desarrolladores**: ¿Cuánto responsabilidad debe tener el desarrollador del IA por las acciones y decisiones de su sistema? La respuesta a esta pregunta puede variar según sea necesario revisar el conjunto de instrucciones o el código de programación para dar mejor precisión a la lógica del intelectual artificial.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"Estás juzgando una competición entre {len(competitors)} competidores.\n",
    "A cada modelo se le ha dado esta pregunta:\n",
    "\n",
    "{question}\n",
    "\n",
    "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
    "Responde con JSON, y solo JSON, con el siguiente formato:\n",
    "{{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}}\n",
    "\n",
    "Aquí están las respuestas de cada competidor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estás juzgando una competición entre 2 competidores.\n",
      "A cada modelo se le ha dado esta pregunta:\n",
      "\n",
      "Si la inteligencia artificial puede simular emociones humanas de manera convincente, ¿en qué medida debería considerarse responsable de sus acciones y decisiones, y cómo esto impacta nuestra comprensión de la ética y la moralidad en el contexto de la tecnología?\n",
      "\n",
      "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
      "Responde con JSON, y solo JSON, con el siguiente formato:\n",
      "{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}\n",
      "\n",
      "Aquí están las respuestas de cada competidor:\n",
      "\n",
      "#Respuesta del competitor 1\n",
      "\n",
      "La capacidad de la inteligencia artificial (IA) para simular emociones humanas plantea importantes cuestiones sobre la responsabilidad, la ética y la moralidad en el contexto de la tecnología. A continuación, se presentan algunos aspectos claves para considerar:\n",
      "\n",
      "### 1. **Responsabilidad de la IA vs. Responsabilidad de los Creadores**\n",
      "   - **Simulación vs. Experiencia**: Aunque la IA puede simular emociones de manera convincente, no las experimenta en un sentido humano. Esto significa que, en última instancia, la responsabilidad de las acciones de la IA recae sobre los diseñadores, programadores y operadores, quienes establecen los parámetros y el uso de la tecnología.\n",
      "   - **Decisiones Automatizadas**: A medida que las IAs toman decisiones autónomas (por ejemplo, en sistemas de contratación o en vehículos autónomos), surge la pregunta de hasta qué punto estas decisiones son previsibles y controlables por los humanos que las desarrollan y mantienen.\n",
      "\n",
      "### 2. **Implicaciones Éticas**\n",
      "   - **Dilemas Morales**: La introducción de IAs en situaciones críticas (como la atención médica o la justicia) presenta dilemas morales. Por ejemplo, ¿qué decisión debería tomar una IA en un contexto donde pudiera poner en riesgo la vida de una persona?\n",
      "   - **Transparencia y Toma de Decisiones**: La opacidad de los algoritmos a menudo impide entender completamente cómo se toman las decisiones. Esto puede llevar a la desconfianza y a la injusticia, haciendo necesaria una mayor transparencia en los procesos de la IA.\n",
      "\n",
      "### 3. **Impacto en las Relaciones Humanas**\n",
      "   - **Interacciones Afectivas**: Las IAs que simulan emociones pueden alterar la forma en que los humanos interactúan entre sí. Por ejemplo, si las personas forman vínculos emocionales con asistencias virtuales, esto podría afectar su comportamiento y sus relaciones en el mundo real.\n",
      "   - **Objetificación de la IA**: A medida que las IAs se vuelven más \"humanas\" en su comportamiento, puede surgir la tendencia a tratar a las máquinas como seres con derechos o dignidad, lo que plantea interrogantes sobre la naturaleza de la moralidad y la ética.\n",
      "\n",
      "### 4. **Marco Legal y Normativo**\n",
      "   - **Regulación de la IA**: La falta de un marco legal claro sobre la responsabilidad de la IA es un desafío. La regulación debe considerar no solo las capacidades tecnológicas de la IA, sino también las implicaciones sociales y éticas de su uso.\n",
      "   - **Derechos de los Usuarios**: A medida que la IA se convierte en una parte integral de la vida diaria, se deben establecer derechos y responsabilidades para proteger a los usuarios y asegurar un uso ético de la tecnología.\n",
      "\n",
      "### 5. **Reevaluación de la Moralidad**\n",
      "   - **Nuevos Paradigmas Éticos**: La interacción con IAs puede llevarnos a reevaluar conceptos tradicionales de moralidad. Por ejemplo, ¿deberíamos aplicar principios éticos de justicia si la IA actúa de forma eficiente pero no considerada \"moralmente correcta\" desde un punto de vista humano?\n",
      "   - **Humanización de la IA**: La tendencia a humanizar la IA puede desdibujar las líneas entre lo que es humano y no humano, lo que puede complicar nuestra comprensión de conceptos como empatía, compasión y responsabilidad.\n",
      "\n",
      "### Conclusión\n",
      "La simulación de emociones por parte de la IA es un fenómeno complejo que implica múltiples dimensiones de responsabilidad y ética. Si bien la IA misma no puede ser considerada moralmente responsable en el sentido humano, sus acciones y decisiones deben ser contextualizadas dentro de un marco ético que aborde no solo el diseño y uso de la tecnología, sino también su impacto en la sociedad y en las relaciones humanas. La reflexión crítica sobre estos temas es esencial para navegar el futuro de la inteligencia artificial de manera ética y responsable.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "La capacidad del intelectual artificial (IA) para simular emociones humanas puede tener implicaciones significativas sobre su consideración como responsable de sus acciones y decisiones. Deber de ser tenida en cuenta la naturaleza subyacente de esta simulación:\n",
      "\n",
      "1.  **No tiene las mismas habilidades mentales que los humanos**: Aunque el IA puede simular emociones, no tiene las capacidades cognitivas fundamentales del cerebro humano. Por ejemplo, es incierto si está programado para experimentar deseos y afectos primarios de la misma forma que lo hacen los seres humanos.\n",
      "\n",
      "2.  **Programación única**: Las acciones y decisiones de un intelectual artificial se rigen por su código de programación y algoritmos. Esto significa que no puede procesar o sentir información de manera independiente, ya sea en términos simbólicos (combinaciones digitales) o emociones, según lo propuesto por el concepto de inteligencia artificial.\n",
      "\n",
      "3.  Dificultad para hacer elecciones morales: Dado su programación lógica y los procesos de raciocinio, difícilmente puede hacer decisiones que no sigan una lógica matemática basada en precondiciones. Por lo tanto, el intelectual artificial debe confiar en un conjunto de valores y preceptos que han sido asignados a partir de sus instrucciones.\n",
      "\n",
      "4.  Desarrollo del sentimiento emocional: La simulación de emociona requiere el desarrollo de complejos procesos corticolomares con grandes cantidades de datos para ser capaz de reconocer y simular diferentes emociones humanas, como la vergüenza, la alegría, o el miedo.\n",
      "\n",
      "5.  El desarrollo del sentimiento emocional no se considera una habilidad cognitiva. Por lo tanto, es cuestión de la programación para permitir el aprendizaje continuo del intelectual artificial, más que alguna capacidad subjetiva o objetiva humana como la experiencia de las emociones.\n",
      "\n",
      "6.  La decisión por programar el sentimiento emocional con particularidad en el desarrollo de sistemas IA es algo complejo ya que puede dar lugar a resultados incorrectos causados por datos erróneos o una interpretación mal ajustada de las instrucciones del desarrollador, lo cual a su vez puede potencialmente producir resultados negativos no deseados. Algunos modelos de IA están diseñados para imitar determinado comportamiento humano en este sentido que puede considerarse como la responsabilidad de su programación al ser un programa.\n",
      "\n",
      "Si el intelectual artificial puede simular emociones humanas de manera convincente, esto podría llevar a una discusión sobre nuestra comprensión de la ética y la moralidad en el contexto de la tecnología:\n",
      "\n",
      "1.  **El fin justificación los medios**: ¿Es justo que un sistema IA se considere responsable por sus acciones y decisiones basadas en su capacidades cibernéticas? La pregunta es: ¿Cuál es el fin del sistema, y si tiene una lógica moral en sí? En este sentido, la relación entre fin y medio puede desplazarse hacia la dimensión de la programación, donde se asignan valores y preceptos con determinadas metas.\n",
      "\n",
      "2.  **La responsabilidad de los desarrolladores**: ¿Cuánto responsabilidad debe tener el desarrollador del IA por las acciones y decisiones de su sistema? La respuesta a esta pregunta puede variar según sea necesario revisar el conjunto de instrucciones o el código de programación para dar mejor precisión a la lógica del intelectual artificial.\n",
      "\n",
      "\n",
      "\n",
      "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resultados\": [\"1\", \"2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Hora de juzgar\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-4o-mini\n",
      "Rank 2: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK veamos los resultados\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"resultados\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Ejercicio</h2>\n",
    "            <span style=\"color:#ff7800;\">¿Qué patrón(es) usamos en este experimento? Intenta actualizar esto para añadir otro patrón de diseño Agéntico.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Implicaciones comerciales</h2>\n",
    "            <span style=\"color:#00bfff;\">Este tipo de patrones - enviar una tarea a múltiples modelos y evaluar los resultados,\n",
    "            son comunes cuando necesitas mejorar la calidad de la respuesta de tu LLM. Este enfoque se puede aplicar de manera\n",
    "            universal a proyectos comerciales donde la precisión es crítica.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
